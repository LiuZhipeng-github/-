# 数据结构与算法第六篇.贪心算法与分治算法

贪心算法（greedy algorithm），这种算法思想更多的是指导设计基础算法。贪心算法的经典应用有，霍夫曼编码（Huffman Coding）、Prim 和 Kruskal 最小生成树算法、Dijkstra单源最短路径算法等。

贪心算法解决问题的正确性虽然很多时候都看起来是显而易见的，但是要严谨地证明算法能够得到最优解，并不是件容易的事。所以，很多时候只需要多举几个例子，看一下贪心算法的解决方案是否真的能得到最优解就可以了。

# 一、如何理解“贪心算法”？

先看一个例子:

有一个可以容纳 100kg 物品的背包，可以装以下 5 种豆子，每种豆子装多少能够让背包中所装物品的总价值最大？

| 物品 | 重量(kg) | 总价值(元) |
| ---- | -------- | ---------- |
| 黄豆 | 100      | 100        |
| 绿豆 | 30       | 90         |
| 红豆 | 60       | 120        |
| 黑豆 | 20       | 80         |
| 青豆 | 50       | 75         |

先计算每周豆子的单价，分别是黄豆1元/kg，绿豆3元/kg，红豆2元/kg，黑豆4元/kg，青豆1.5元/kg，单价从高到低排序依次是：黑豆、绿豆、红豆、青豆、黄豆。所以先从单价最高的装，20kg 黑豆、30kg 绿豆、50kg 红豆。

这个问题的解决思路本质上就是贪心算法。

贪心算法适合解决的问题：

**1.定义了限制值和期望值，希望从中选出几个数据，在满足限制值的情况下，期望值最大。**

类比到刚刚的例子，限制值就是重量不能超过 100kg，期望值就是物品的总价值。这组数据就是 5 种豆子。我们从中选出一部分，满足重量不超过 100kg，并且总价值最大。

**2.每次都可以选择当前情况下在对限制值同等贡献量的情况下，对期望值贡献最大的数据。**

类比到刚刚的例子，每次都从剩下的豆子里面，选择单价最高的，也就是重量相同的情况下，对价值贡献最大的豆子。

**3.举几个例子判断贪心算法产生的结果是否最优**

严格地证明贪心算法的正确性，是非常复杂的，需要涉及比较多的数学推理。大部分能用贪心算法解决的问题，贪心算法的正确性都是显而易见的，也不需要严格的数学推导证明。

贪心算法解决问题的思路，并不总能给出最优解：

比如在一个有权图中，从顶点 S 开始，找一条到顶点 T 的最短路径（路径中边的权值和最小）。贪心算法的解决思路是，每次都选择一条跟当前顶点相连的权最小的边，直到找到顶点 T。按照这种思路，求出的最短路径是 S->A->E->T，路径长度是 1+4+4=9。但路径 S->B->D->T 才是最短路径，路径长度是2+2+2=6。

![1570501134514](%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E4%B9%8B%E7%AC%AC%E4%B8%83%E7%AF%87.%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95%E4%B8%8E%E5%88%86%E6%B2%BB%E7%AE%97%E6%B3%95.assets/1570501134514.png)

在这个问题上，贪心算法不工作的主要原因是，前面的选择，会影响后面的选择。如果第一步从顶点 S 走到顶点 A，那接下来面对的顶点和边，跟第一步从顶点 S 走到顶点 B，是完全不同的。所以，即便我们第一步选择最优的走法（边最短），但有可能因为这一步选择，导致后面每一步的选择都很糟糕，最终也就无缘全局最优解了。

# 二、贪心算法实例分析

## 2.1 分糖果

有 m 个糖果和 n 个孩子，但是m<n。

每个糖果的大小不等，这 m 个糖果的大小分别是 s1，s2，s3，……，sm。

每个孩子对糖果大小的需求不一样的，只有糖果的大小大于等于孩子的对糖果大小的需求的时候，孩子才得到满足。这 n 个孩子对糖果大小的需求分别是 g1，g2，g3，……，gn。

**如何分配糖果，能尽可能满足最多数量的孩子？**

这个问题可抽象成，从 n 个孩子中，抽取一部分孩子分配m个糖果，让满足的孩子的个数（期望值）是最大的。

**限制值**是糖果个数 m，**期望**是满足的孩子个数最大。

贪心算法解决思路：

对于一个孩子来说，如果小的糖果可以满足，就没必要用更大的糖果，这样更大的就可以留给其他对糖果大小需求更大的孩子。

另一方面，对糖果大小需求小的孩子更容易被满足。

因为满足一个需求大的孩子跟满足一个需求小的孩子，对期望值的贡献是一样的。

所以从需求小的孩子开始分配糖果。

故每次从剩下的孩子中，找出对糖果大小需求最小的，然后发给他剩下的糖果中能满足他的最小的糖果，这样得到的分配方案，也就是满足的孩子个数最多的方案。

## 2.2 钱币找零

假设有 1 元、2 元、5 元、10 元、20 元、50 元、100 元这些面额的纸币，它们的张数分别是 c1、c2、c5、c10、c20、c50、c100。现在要用这些钱来支付 K 元，最少要用多少张纸币呢？

**贪心算法的解决思路：**

希望在相同纸币数目的情况下，多贡献点金额，这样就可以让纸币数更少。

即先用面值最大的来支付，如果不够，就继续用更小一点面值的，以此类推，最后剩下的用 1 元来补齐。

## 2.3 区间覆盖

假设有 n 个区间，区间的起始端点和结束端点分别是 [l1, r1]，[l2, r2]，[l3, r3]，……，[ln, rn]。从这 n 个区间中选出一部分区间，这部分区间满足两两不相交（端点相交的情况不算相交），最多能选出多少个区间呢？
$$
\begin{array}{l}{\text { 区间: }[6,8][2,4][3,5][1,5][1,9][8,10]} \\ {\text { 不相交区间: }[2,4][6,8][8,10]}\end{array}
$$
贪心算法的解决思路：假设这 n 个区间中最左端点是 lmin，最右端点是 rmax。

这个问题就抽象为选择几个不相交的区间，从左到右将 [lmin, rmax] 覆盖上。

先按照起始端点从小到大的顺序对这 n 个区间排序。

每次选择左端点跟前面的已经覆盖的区间不重合，右端点又尽量小的区间：

 ![1570501292623](%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E4%B9%8B%E7%AC%AC%E4%B8%83%E7%AF%87.%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95%E4%B8%8E%E5%88%86%E6%B2%BB%E7%AE%97%E6%B3%95.assets/1570501292623.png)

## 2.4如何用贪心算法实现霍夫曼编码？

假设有一个包含1000个字符的文件，每个字符占 1 个 byte（1byte=8bits），存储这 1000 个字符就一共需要 8000bits。

假设通过统计分析发现，这 1000 个字符中只包含 6 种不同字符分别是 a、b、c、d、e、f。而 3 个二进制位（bit）就可以表示 8 个不同的字符，所以，为了尽量减少存储空间每个字符只用 3 个二进制位来表示。那存储这 1000 个字符就减少到 3000bits ：

```
a(000)、b(001)、c(010)、d(011)、e(100)、f(101)
```

而霍夫曼编码不仅会考察文本中有多少个不同字符，还会考察每个字符出现的频率，根据频率的不同，选择不同长度的编码。霍夫曼编码是一种十分有效的编码方法，广泛用于数据压缩中，其压缩率通常在 20%～90% 之间。

根据贪心的思想，把出现频率比较多的字符，用稍微短一些的编码；出现频率比较少的字符，用稍微长一些的编码。

对于等长的编码来说，每次读取 固定二进制码，然后翻译成对应的字符。但霍夫曼编码是不等长的，它要求各个字符的编码之间，不会出现某个编码是另一个编码前缀的情况。

假设这 6 个字符出现的频率从高到低依次是 a、b、c、d、e、f。

把它们编码下面这个样子，任何一个字符的编码都不是另一个的前缀：

![1570501375909](%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E4%B9%8B%E7%AC%AC%E4%B8%83%E7%AF%87.%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95%E4%B8%8E%E5%88%86%E6%B2%BB%E7%AE%97%E6%B3%95.assets/1570501375909.png)

在解压缩的时候，每次会读取尽可能长的可解压的二进制串，所以在解压缩的时候也不会歧义。经过这种编码压缩之后，这 1000 个字符只需要 2100bits 就可以了。

**如何根据字符出现频率的不同，给不同的字符进行不同长度的编码呢？**

把每个字符看作一个节点，并且辅带着把频率放到优先级队列中。从队列中取出频率最小的两个节点 A、B，然后新建一个节点 C，把频率设置为两个节点的频率之和，并把这个新节点 C 作为节点 A、B 的父节点。最后再把 C 节点放入到优先级队列中。重复这个过程，直到队列中没有数据。

![1570501407500](%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E4%B9%8B%E7%AC%AC%E4%B8%83%E7%AF%87.%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95%E4%B8%8E%E5%88%86%E6%B2%BB%E7%AE%97%E6%B3%95.assets/1570501407500.png)

然后给每一条边加上一个权值，指向左子节点的边统统标记为 0，指向右子节点的边统统标记为 1，那从根节点到叶节点的路径就是叶节点对应字符的霍夫曼编码。

![1570501425196](%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E4%B9%8B%E7%AC%AC%E4%B8%83%E7%AF%87.%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95%E4%B8%8E%E5%88%86%E6%B2%BB%E7%AE%97%E6%B3%95.assets/1570501425196.png)

再以原始集合的值是[2,3,4,4,5,7]为例：

 ![img](%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E4%B9%8B%E7%AC%AC%E4%B8%83%E7%AF%87.%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95%E4%B8%8E%E5%88%86%E6%B2%BB%E7%AE%97%E6%B3%95.assets/Huffman_algorithm.gif)

 第一步：从原始集合中取出最小的两个值并将这两个值从原始集合中剔除，这两个最小的值相加得到一个新的值并加入原始集合，这一步执行之后原始集合就变成：[⑤,4,4,5,7]

第二步：从更新后的集合中再取最小的两个值并剔除，同样相加得到新值加入到集合。这一步执行之后集合就变成：[⑤,⑧,5,7]

第三步，重复以上步骤，结果是：[⑩,⑧,7]
第四步，结果是：[⑩,(15)]
最后一步，结果是： [(25)]

编码结果：

 ![img](%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E4%B9%8B%E7%AC%AC%E4%B8%83%E7%AF%87.%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95%E4%B8%8E%E5%88%86%E6%B2%BB%E7%AE%97%E6%B3%95.assets/TABLE8.JPG) 

# 三、分治算法

分治算法用四个字概括就是“分而治之”，将原问题划分成 n 个规模较小而结构与原问题相似的子问题，递归地解决这些子问题，然后再合并其结果，就得到原问题的解。

分治算法的典型的两种应用场景，一个是用来指导编码，降低问题求解的时间复杂度；另一个是解决海量数据处理问题。比如 MapReduce 本质上就是利用了分治思想。

## 如何理解分治算法？

分治算法（divide and conquer）的核心思想就是分而治之 ，将原问题划分成 n 个规模较小，并且结构与原问题相似的子问题，递归地解决这些子问题，然后再合并其结果，就得到原问题的解。

分治算法一般都比较适合用递归来实现，**分治算法是一种处理问题的思想，递归是一种编程技巧**。

分治算法的递归实现中，每一层递归都会涉及这样三个操作：

- 分解：将原问题分解成一系列子问题；
- 解决：递归地求解各个子问题，若子问题足够小，则直接求解；
- 合并：将子问题的结果合并成原问题。

分治算法能解决的问题，一般需要满足下面这几个条件：

- 原问题与分解成的小问题具有相同的模式；
- 原问题分解成的子问题可以独立求解，子问题之间没有相关性，这一点是分治算法跟动态规划的明显区别；
- 具有分解终止条件，当问题足够小时，可以直接求解；
- 可以将子问题合并成原问题，而这个合并操作的复杂度不能太高，否则就起不到减小算法总体复杂度的效果了。

# 四、分治算法应用举例

## 4.1 求数组的逆序对个数

在排序算法里，我们用有序度来表示一组数据的有序程度，用逆序度表示一组数据的无序程度。

假设有 n 个数据，数据从小到大排列后，那完全有序的数据的有序度就是 n(n-1)/2，逆序度等于 0；

相反，倒序排列的数据的有序度就是 0，逆序度是 n(n-1)/2。

其他情况通过计算有序对或者逆序对的个数，来表示数据的有序度或逆序度。
$$
\begin{array}{l}2,4,3,1,5,6 \quad 逆序对个数:4 \\ {(2,1)(4,3)(4,1)(3,1)}\end{array}
$$
计算逆序对个数最笨的方法是，拿每个数字跟它后面的数字比较，看有几个比它小的，把比它小的数字个数记作 k。把每个数字都考察一遍之后，然后对每个数字对应的 k 值求和，最后得到的总和就是逆序对个数。这样操作的时间复杂度是 $O(n^2)$。

**用分治的思想来求数组 A 的逆序对个数:**

将数组分成前后两半 A1 和 A2，分别计算 A1 和 A2 的逆序对个数 K1 和 K2，然后再计算 A1 与 A2 之间的逆序对个数 K3。那数组 A 的逆序对个数就等于 K1+K2+K3。

使用分治算法其中一个要求是，子问题合并的代价不能太大，否则就起不了降低时间复杂度的效果了。

计算两个子问题 A1 与 A2 之间的逆序对个数，可借助归并排序算法的归并思想。

在两个子数组合并的过程中，就可以计算两个子数组的逆序对个数了：

![1570503357732](%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E4%B9%8B%E7%AC%AC%E4%B8%83%E7%AF%87.%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95%E4%B8%8E%E5%88%86%E6%B2%BB%E7%AE%97%E6%B3%95.assets/1570503357732.png)

## 4.2 求距离最近的两个点

二维平面上有 n 个点，如何快速计算出两个距离最近的点对？

思路：

求n个点中距离最近的两个点，可以将n个点划分为两部分，分别求出n/2个点中最近的两个点，然后再从这4个点中求出距离最近的两个点。

python实现代码：

```python
import sys, math
from typing import Tuple


def get_distance(p1: Tuple, p2: Tuple):
    distance = math.sqrt((p1[0] - p2[0]) ** 2 + (p1[1] - p2[1]) ** 2)
    # print(p1, p2, distance)
    return distance


def min_distance_point(nums, p, r) -> list:
    if r - p < 2: return nums[p:r + 1]
    mid = (p + r) >> 1
    pots = min_distance_point(nums, p, mid) + min_distance_point(nums, mid + 1, r)
    n = len(pots)
    min_distance = sys.maxsize
    pot1, pot2 = None, None
    for i in range(n - 1):
        for j in range(i + 1, n):
            distance = get_distance(pots[i], pots[j])
            if distance < min_distance:
                min_distance = distance
                pot1, pot2 = pots[i], pots[j]
    return [pot1, pot2]


nums = [(5, 0), (4, 2), (3, 2), (3, 1), (3, 4), (3, 6), (8, 7), (2, 3), (1, 1)]
pot = min_distance_point(nums, 0, len(nums) - 1)
p1, p2 = pot
print(pot, get_distance(p1, p2))
```

## 4.3 分治思想在海量数据处理中的应用

比如，给 10GB 的订单文件按照金额排序，机器的内存可能只有 2GB，无法一次性加载到内存。

要解决这种数据量大到内存装不下的问题，可以将海量的数据集合根据某种方法，划分为几个小的数据集合，每个小的数据集合单独加载到内存来解决，然后再将小数据集合合并成大数据集合。

先扫描一遍订单，根据订单的金额，将 10GB 的文件划分为几个金额区间。比如订单金额为 1 到 100 元的放到一个小文件，101 到 200 之间的放到另一个文件，以此类推。这样每个小文件都可以单独加载到内存排序，最后将这些有序的小文件合并，就是最终有序的 10GB 订单数据了。

如果订单数据存储在类似 GFS 这样的分布式系统上，当 10GB 的订单被划分成多个小文件的时候，每个文件可以并行加载到多台机器上处理，最后再将结果合并在一起。不过，数据的存储与计算所在的机器必须是同一个或者在网络中靠的很近（比如一个局域网内，数据存取速度很快），否则就会因为数据访问的速度，导致整个处理过程不但不会变快，反而有可能变慢。



## 4.4 为什么说 MapReduce 的本质就是分治思想？

如果我们要处理的数据是 1T、10T、100T 这样子的，那一台机器处理的效率肯定是非常低的。而对于谷歌搜索引擎来说，网页爬取、清洗、分析、分词、计算权重、倒排索引等等各个环节中，都会面对如此海量的数据（比如网页）。所以，利用集群并行处理显然是大势所趋。

把任务拆分到多台机器上来处理，拆分之后的小任务之间互不干扰，独立计算，最后再将结果合并，就是分治思想。

实际上，MapReduce 框架只是一个任务调度器，底层依赖 GFS 来存储数据，依赖 Borg 管理机器。它从 GFS 中拿数据，交给 Borg 中的机器执行，并且时刻监控机器执行的进度，一旦出现机器宕机、进度卡壳等，就重新从 Borg 中调度一台机器执行。

尽管 MapReduce 的模型非常简单，但是在 Google 内部应用非常广泛。它除了可以用来处理这种数据与数据之间存在关系的任务，比如 MapReduce 的经典例子，统计文件中单词出现的频率。除此之外，它还可以用来处理数据与数据之间没有关系的任务，比如对网页分析、分词等，每个网页可以独立的分析、分词，而这两个网页之间并没有关系。网页几十亿、上百亿，如果单机处理，效率低下就可以利用 MapReduce 提供的高可靠、高性能、高容错的并行计算框架，并行地处理这几十亿、上百亿的网页。

